People look at an image and immediately know what objects are in the image, where they are and how they interact with each other. The human visual system is fast and accurate, so that we can perform complex tasks such as driving a car with little conscious thinking. Fast, accurate algorithms for object detection would enable computers to drive cars without specialized sensors, to provide real-time, sane information to human users and to unlock the potential for general, responsive robot systems. The current detection systems reuse classifications to perform detection. To detect an object, these systems use a classification for that object and evaluate this at different locations and scales in a test image. Systems such as models for deformable parts, DPM, use a sliding window approach, where the classifier is executed at evenly distributed locations across the entire image. More recent approaches, such as R-CNN, use a regio-proposal method to first generate potential limitations in an image and then execute a classifier on these pre-set frames. After classification, post-processing is used to refine the surrounding frames, eliminate double detections and score the frames again based on other objects. These complex pipelines are slow and difficult to optimize because each individual part must be trained individually. We re-formulate object detection as a single regression problem, straight from image pixels to boundary frame coordinates and class probabilities. With our system, you only have to look at an image of YOLO once to predict which objects are present and where they are. YOLO is extremely simple. A single convolutional network predicts several boundary cases and class probabilities for those cases. YOLO trains on entire images and optimizes direct detection performance. This uniform model has several advantages compared to traditional methods for object detection. First of all, YOLO is extremely fast. Because we consider detection as a regression problem, we don't need a complex pipeline. During the test, we simply run our neural network on a new image to predict detections. Our base network runs at 45 frames per second without batch processing on a Titan X GPU and a fast version runs at more than 150 FPS. This means that we can process streaming video in real-time with a latency of less than 25 milliseconds. In addition, YOLO reaches the average accuracy of other real-time systems more than twice. For a demo of our system running in real-time on a webcam, you can visit our project page at pjready.com. Secondly, YOLO globally reasons about the image when making predictions. In contrast to techniques based on sliding windows and region proposals, YOLO sees the entire image during the training and test period, so that it implicitly codes contextual information about classes and their appearance.