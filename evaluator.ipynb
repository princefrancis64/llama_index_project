{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELEVANCY EVALUATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio \n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import DatasetGenerator, RelevancyEvaluator,FaithfulnessEvaluator,EvaluationResult\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Response\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the data from the directory data\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prince\\Desktop\\Projects\\llama\\myenv\\Lib\\site-packages\\llama_index\\core\\evaluation\\dataset_generation.py:212: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n"
     ]
    }
   ],
   "source": [
    "# Use a DatasetGenerator to build train_dataset and test_dataset\n",
    "data_generator = DatasetGenerator.from_documents(documents,num_questions_per_chunk=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prince\\Desktop\\Projects\\llama\\myenv\\Lib\\site-packages\\llama_index\\core\\evaluation\\dataset_generation.py:309: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "## Generate questions\n",
    "eval_questions = data_generator.generate_questions_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the main advantage of fast and accurate algorithms for object detection?',\n",
       " 'How do current detection systems typically work?',\n",
       " 'What is the sliding window approach used by systems like models for deformable parts (DPM)?',\n",
       " 'How does the R-CNN approach differ from the sliding window approach?',\n",
       " 'What is the purpose of post-processing in object detection systems?',\n",
       " 'Why are complex pipelines for object detection difficult to optimize?',\n",
       " 'How is object detection re-formulated in the YOLO system?',\n",
       " 'What are the advantages of YOLO compared to traditional methods for object detection?',\n",
       " 'How fast is the base network of YOLO able to run during testing?',\n",
       " 'What is the average accuracy of YOLO compared to other real-time systems?',\n",
       " 'How does YOLO differ from techniques based on sliding windows and region proposals in terms of reasoning about the image?',\n",
       " 'What is the significance of considering detection as a regression problem in YOLO?',\n",
       " 'How does YOLO optimize direct detection performance?',\n",
       " 'What is the purpose of the region-proposal method used in some object detection systems?',\n",
       " 'How does YOLO train on entire images for object detection?',\n",
       " 'What is the role of the human visual system in object detection?',\n",
       " 'How does YOLO predict object boundaries and class probabilities?',\n",
       " 'What is the potential impact of fast and accurate object detection algorithms on autonomous vehicles?',\n",
       " 'How does YOLO eliminate the need for a complex pipeline in object detection?',\n",
       " 'What is the significance of YOLO being able to process streaming video in real-time with low latency?',\n",
       " \"How does YOLO's approach to object detection differ from traditional methods in terms of speed?\",\n",
       " 'What is the main drawback of current detection systems that YOLO aims to address?',\n",
       " 'How does YOLO optimize performance during testing?',\n",
       " 'How does YOLO leverage contextual information about classes and their appearance during training and testing?',\n",
       " \"How does YOLO's approach to object detection contribute to the potential for general, responsive robot systems?\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### These questions would be considered as our train set\n",
    "eval_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "evaluator_gpt4 = RelevancyEvaluator(llm=gpt4)\n",
    "# create vector index\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define jupyter display function\n",
    "def display_eval_df(\n",
    "    query:str, response:Response, eval_result:EvaluationResult\n",
    ") -> None:\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Query\": query,\n",
    "            \"Response\": str(response),\n",
    "            \"Source\": response.source_nodes[0].node.text[:1000] + \"...\",\n",
    "            \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n",
    "            \"Reasoning\": eval_result.feedback,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATE RESPONSE FOR THE TRAIN SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "response_vector = query_engine.query(eval_questions[1])\n",
    "eval_result = evaluator_gpt4.evaluate_response(\n",
    "    query=eval_questions[1], response=response_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_17b6b_row0_col1, #T_17b6b_row0_col2 {\n",
       "  inline-size: 600px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_17b6b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_17b6b_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
       "      <th id=\"T_17b6b_level0_col1\" class=\"col_heading level0 col1\" >Response</th>\n",
       "      <th id=\"T_17b6b_level0_col2\" class=\"col_heading level0 col2\" >Source</th>\n",
       "      <th id=\"T_17b6b_level0_col3\" class=\"col_heading level0 col3\" >Evaluation Result</th>\n",
       "      <th id=\"T_17b6b_level0_col4\" class=\"col_heading level0 col4\" >Reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_17b6b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_17b6b_row0_col0\" class=\"data row0 col0\" >How do current detection systems typically work?</td>\n",
       "      <td id=\"T_17b6b_row0_col1\" class=\"data row0 col1\" >Current detection systems typically reuse classifications to perform detection. They use a classification for an object and evaluate it at different locations and scales in a test image. Systems like models for deformable parts (DPM) use a sliding window approach, executing the classifier at evenly distributed locations across the entire image. More recent approaches, such as R-CNN, use a region-proposal method to generate potential limitations in an image and then execute a classifier on these pre-set frames. Post-processing is used after classification to refine surrounding frames, eliminate double detections, and score the frames again based on other objects.</td>\n",
       "      <td id=\"T_17b6b_row0_col2\" class=\"data row0 col2\" >People look at an image and immediately know what objects are in the image, where they are and how they interact with each other. The human visual system is fast and accurate, so that we can perform complex tasks such as driving a car with little conscious thinking. Fast, accurate algorithms for object detection would enable computers to drive cars without specialized sensors, to provide real-time, sane information to human users and to unlock the potential for general, responsive robot systems. The current detection systems reuse classifications to perform detection. To detect an object, these systems use a classification for that object and evaluate this at different locations and scales in a test image. Systems such as models for deformable parts, DPM, use a sliding window approach, where the classifier is executed at evenly distributed locations across the entire image. More recent approaches, such as R-CNN, use a regio-proposal method to first generate potential limitations in an ...</td>\n",
       "      <td id=\"T_17b6b_row0_col3\" class=\"data row0 col3\" >Pass</td>\n",
       "      <td id=\"T_17b6b_row0_col4\" class=\"data row0 col4\" >YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25c159b6850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_eval_df(eval_questions[1], response_vector, eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAITHFULNESS EVALUATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "response = query_engine.query(eval_questions[1])\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "print(str(eval_result.passing))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
